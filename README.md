# Image-moderator-documentation
**Image moderator** is a web app that returns probability scores on the likelihood that an image contains concepts such as gore, drugs, explicit or suggestive nudity. It determines if images are â€œsafe".

This website is deployed on heroku. 

You can access the live website using the link - https://imagemoderator.herokuapp.com/

You can access the frontend files repository of this project using this link - https://github.com/singhalaman23/Image-moderator-frontend .

You can access the backend files repository of this project using the link - https://github.com/singhalaman23/Image-moderator-backend .

***Note: The frontend and backend repositories mentioned above were used only for the deployment purpose in Heroku. This current repository is for documentation purpose, so that if the live link doesn't work due to any reason, then you can see how the website looked when it was live.***

Here are few snapshots from the website : 

>First look of the website upon loading.

![Image 1](https://github.com/singhalaman23/Image-moderator-documentation/blob/main/ImageModIMG1.png?raw=true)

>User copied and pasted URL of a random image and clicked on DETECT button.

![Image 2](https://github.com/singhalaman23/Image-moderator-documentation/blob/main/ImageModIMG2.png?raw=true)

>After clicking on DETECT, the user clicked on RESULT button.

![Image 3](https://github.com/singhalaman23/Image-moderator-documentation/blob/main/ImageModIMG3.png?raw=true)

>The footer of the website.

![Image 4](https://github.com/singhalaman23/Image-moderator-documentation/blob/main/ImageModIMG4.png?raw=true)


Here are few snapshots of the website from a mobile device

![MobImage 1](https://github.com/singhalaman23/Image-moderator-documentation/blob/main/MobileImageModIMG1.png?raw=true)

![MobImage 1](https://github.com/singhalaman23/Image-moderator-documentation/blob/main/MobileImageModIMG2.png?raw=true)



